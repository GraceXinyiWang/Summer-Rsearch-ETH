{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import shutil\n",
    "import socket\n",
    "import GPUtil as GPU\n",
    "import imageio\n",
    "import tensorflow as tf\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import elasticdeform\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from PIL import ImageFile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from skimage import exposure\n",
    "\n",
    "\n",
    "from tensorflow import ones_like, equal\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dropout, Lambda\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from skimage import color, io, img_as_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noteBookName = \"u_net_uterus_seg.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "\n",
    "DATA_DIR = Path('../data/michael/clean')\n",
    "\n",
    "IMG_WIDTH = 240  # 180 # 240\n",
    "IMG_HEIGHT = 240 # 180 # 240\n",
    "IMG_CHANNELS = 1\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "NUM_FOLDS = 10\n",
    "VAL_SPLIT = 0.1\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='skimage')\n",
    "\n",
    "seed = 42\n",
    "random.seed = seed\n",
    "np.random.seed = seed\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first available gpu\n",
    "if \"fabian\" in str(socket.gethostname()):\n",
    "    gpu_str = str(GPU.getFirstAvailable(order=\"load\")[0])\n",
    "    print(\"local gpu: \" + gpu_str)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_str\n",
    "else:\n",
    "    gpu_str = str(GPU.getFirstAvailable(order=\"load\", maxLoad=10**-6, maxMemory=10**-1)[0]) \n",
    "    print(\"server gpu: \" + gpu_str)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_frame(filename, frame_idx=None):\n",
    "    \n",
    "    video = get_echo_video(filename)\n",
    "    n_frames = video.shape[0]\n",
    "    \n",
    "    if frame_idx is None:\n",
    "        frame_idx = random.randrange(0, n_frames)\n",
    "    \n",
    "    else:\n",
    "        if frame_idx >= n_frames:\n",
    "            frame_idx = n_frames-1\n",
    "        \n",
    "    frame = video[frame_idx, ...]\n",
    "    \n",
    "    # only return first channel (all are the same)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_as_array(file, do_resize=False):\n",
    "    \n",
    "    # Create a VideoCapture object and read from input file\n",
    "    # If the input is the camera, pass 0 instead of the video file name\n",
    "    cap = cv2.VideoCapture(file)\n",
    "\n",
    "    # Check if camera opened successfully\n",
    "    if (cap.isOpened()== False): \n",
    "        print(\"Error opening video stream or file\")\n",
    "\n",
    "    # Read until video is completed\n",
    "    frames = []\n",
    "    while(cap.isOpened()):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "            \n",
    "            if do_resize:\n",
    "                frame = resize(frame, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True) \n",
    "            \n",
    "            frames.append(frame)\n",
    "\n",
    "        # Break the loop\n",
    "        else: \n",
    "            break\n",
    "\n",
    "    # When everything done, release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    return np.asarray(frames).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(original_path, annotated_path, do_resize=True):\n",
    "    \n",
    "    original = get_video_as_array(original_path)\n",
    "    annotated = get_video_as_array(annotated_path)\n",
    "    \n",
    "    if original.shape != annotated.shape:\n",
    "        print(\"Shapes do not match!\")\n",
    "        print(original.shape,  annotated.shape)\n",
    "        \n",
    "        return None, None\n",
    "    \n",
    "    original_frames = {}\n",
    "    annotated_frames = {}\n",
    "    \n",
    "    for i in range(annotated.shape[0]):\n",
    "\n",
    "        frame = annotated[i, ...]\n",
    "\n",
    "        annotated_frame = ((frame[..., 0] < 10) & (frame[..., 1] < 10) & (frame[..., 2] > 150))        \n",
    "        if annotated_frame.any():\n",
    "            \n",
    "            \n",
    "            if do_resize:\n",
    "                original_frame = resize(original[i, ...], \n",
    "                                        (IMG_HEIGHT, IMG_WIDTH), \n",
    "                                        mode='constant', \n",
    "                                        preserve_range=True) \n",
    "                annotated_frame = resize(annotated_frame, \n",
    "                                         (IMG_HEIGHT, IMG_WIDTH), \n",
    "                                         mode='constant', \n",
    "                                         preserve_range=True)\n",
    "            else:\n",
    "                original_frame = original[i, ...]\n",
    "                annotated_frame = annotated_frame\n",
    "                \n",
    "            \n",
    "            original_frame = original_frame[..., 0].astype(np.uint8)\n",
    "            annotated_frame = annotated_frame.astype(np.uint8)\n",
    "            \n",
    "                \n",
    "            \n",
    "            original_frames[str(i)] = original_frame\n",
    "            annotated_frames[str(i)] = annotated_frame\n",
    "                \n",
    "                \n",
    "    return original_frames, annotated_frames   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_dir):\n",
    "    \n",
    "    patients = os.listdir(data_dir)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i, p in tqdm(enumerate(patients), total=len(patients)):\n",
    "        pat_file_dic = {}\n",
    "        for f in os.listdir(data_dir / p):\n",
    "            path = str(data_dir / p / f)\n",
    "            pat_file_dic['pat'] = p\n",
    "\n",
    "            # check for correct files\n",
    "            if p.replace('case', '') in f:\n",
    "                if 'rendered.mp4' in f:\n",
    "                    pat_file_dic['orig_video_path'] = path\n",
    "                if 'annotated.mp4' in f:\n",
    "                    pat_file_dic['anno_video_path'] = path\n",
    "\n",
    "        if len(pat_file_dic.keys()) == 3:\n",
    "\n",
    "            o_frames, a_frames = get_frames(pat_file_dic['orig_video_path'],\n",
    "                                                                     pat_file_dic['anno_video_path'])\n",
    "\n",
    "            if o_frames is None:\n",
    "                continue\n",
    "\n",
    "            pat_file_dic['original_frames'] = o_frames\n",
    "            pat_file_dic['annotated_frames'] = a_frames\n",
    "\n",
    "            data.append(pat_file_dic)\n",
    "        else:\n",
    "            print(f\"Problem with pat: {p}. Incorrect file name.\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(frame, n_classes=2):\n",
    "    n_values = np.max(frame) + 1\n",
    "    one_hot = np.eye(n_values)[frame]\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_frame(frame, label, is_train=True):\n",
    "    \n",
    "    frame = np.asarray(frame, dtype=np.float32)\n",
    "    label =  np.asarray(label, dtype=np.float32)\n",
    "    if np.max(frame) > 1:\n",
    "        frame /= 255.0\n",
    "    \n",
    "    if is_train:\n",
    "        # contrast augmentation\n",
    "        k_scale = random.randrange(6, 9, 1)\n",
    "        clip_limit = random.randrange(2, 5, 1)/100\n",
    "        \n",
    "        # TODO: add brightness augmentation?\n",
    "        \n",
    "    else:\n",
    "        k_scale = 8\n",
    "        clip_limit = 0.03\n",
    "      \n",
    "    #frame = ndimage.median_filter(frame, size=3) # (makes it slower)\n",
    "    \n",
    "    # normalize frame\n",
    "    frame = exposure.equalize_adapthist(frame, \n",
    "                                        clip_limit=clip_limit, \n",
    "                                        kernel_size=[int(d/k_scale) for d in frame.shape])\n",
    "    \n",
    "    if is_train:\n",
    "        \n",
    "        # augment 50% of the frames\n",
    "        if random.random() > 0.5:\n",
    "            frame, label = elasticdeform.deform_random_grid([frame, label], \n",
    "                                                     sigma=10, \n",
    "                                                     points=3,\n",
    "                                                     rotate=random.randrange(-15,15, 1), \n",
    "                                                     zoom=random.randrange(9,14, 1)/10)\n",
    "\n",
    "    label = np.asarray(label > 0.5, dtype=np.uint8)\n",
    "    \n",
    "    return frame, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data, is_train=True):\n",
    "    \n",
    "    def _yield_frames():\n",
    "        for d in data:\n",
    "            for k in d['original_frames'].keys():\n",
    "\n",
    "                original_frame = d['original_frames'][k]\n",
    "                annotated_frame = d['annotated_frames'][k]\n",
    "                \n",
    "                original_frame, annotated_frame = augment_frame(original_frame, annotated_frame, is_train=is_train)\n",
    "                \n",
    "                original_frame = original_frame[..., np.newaxis]\n",
    "                annotated_frame = to_one_hot(annotated_frame, n_classes=NUM_CLASSES)\n",
    "                \n",
    "                if annotated_frame.shape[-1] != 2:\n",
    "                    continue\n",
    "                \n",
    "\n",
    "                yield original_frame, annotated_frame\n",
    "        \n",
    "    \n",
    "    if is_train:\n",
    "        ds = tf.data.Dataset.from_generator(_yield_frames,\n",
    "                                             output_signature=(\n",
    "                                                 tf.TensorSpec(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), \n",
    "                                                               dtype=tf.float32),\n",
    "                                                 tf.TensorSpec(shape=(IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES), \n",
    "                                                               dtype=tf.float32)))           \n",
    "        ds = ds.repeat().shuffle(48)\n",
    "        ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_generator(_yield_frames,\n",
    "                                             output_signature=(\n",
    "                                                 tf.TensorSpec(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), \n",
    "                                                               dtype=tf.float32),\n",
    "                                                 tf.TensorSpec(shape=(IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES), \n",
    "                                                               dtype=tf.float32)))           \n",
    "        ds = ds.batch(1).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_binary_crossentropy(w1, w2):\n",
    "    '''\n",
    "    w1 and w2 are the weights for the two classes.\n",
    "    Computes weighted binary crossentropy\n",
    "    Use like so:  model.compile(loss=weighted_binary_crossentropy(), optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    '''\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        # avoid absolute 0\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        ones = ones_like(y_true)\n",
    "        msk = equal(y_true, ones)\n",
    "        # tensor of booleans of length == y_true; true means that the true class is 1\n",
    "\n",
    "        res, _ = tf.map_fn(lambda x: (mul(-tf.math.log(x[0]), w1) if x[1] is True\n",
    "                                      else mul(-tf.math.log(1 - x[0]), w2), x[1]),\n",
    "                           (y_pred, msk), dtype=(tf.float32, tf.bool))\n",
    "\n",
    "        return res\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build U-Net model\n",
    "def build_model(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS):\n",
    "    \n",
    "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "\n",
    "    c1 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (inputs)\n",
    "    c1 = BatchNormalization()(c1)\n",
    "    c1 = Dropout(0.1) (c1)\n",
    "    c1 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c1)\n",
    "    c1 = BatchNormalization()(c1)\n",
    "    p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "    c2 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p1)\n",
    "    c2 = BatchNormalization()(c2)\n",
    "    c2 = Dropout(0.1) (c2)\n",
    "    c2 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c2)\n",
    "    c2 = BatchNormalization()(c2)\n",
    "    p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "    c3 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p2)\n",
    "    c3 = BatchNormalization()(c3)\n",
    "    c3 = Dropout(0.2) (c3)\n",
    "    c3 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c3)\n",
    "    c3 = BatchNormalization()(c3)\n",
    "    p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "    c4 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p3)\n",
    "    c4 = BatchNormalization()(c4)\n",
    "    c4 = Dropout(0.2) (c4)\n",
    "    c4 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c4)\n",
    "    c4 = BatchNormalization()(c4)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "\n",
    "    c5 = Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p4)\n",
    "    c5 = BatchNormalization()(c5)\n",
    "    c5 = Dropout(0.3) (c5)\n",
    "    c5 = Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c5)\n",
    "    c5 = BatchNormalization()(c5)\n",
    "\n",
    "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u6)\n",
    "    c6 = BatchNormalization()(c6)\n",
    "    c6 = Dropout(0.2) (c6)\n",
    "    c6 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c6)\n",
    "    c6 = BatchNormalization()(c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u7)\n",
    "    c7 = BatchNormalization()(c7)\n",
    "    c7 = Dropout(0.2) (c7)\n",
    "    c7 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c7)\n",
    "    c7 = BatchNormalization()(c7)\n",
    "\n",
    "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u8)\n",
    "    c8 = BatchNormalization()(c8)\n",
    "    c8 = Dropout(0.1) (c8)\n",
    "    c8 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c8)\n",
    "    c8 = BatchNormalization()(c8)\n",
    "\n",
    "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u9)\n",
    "    c9 = BatchNormalization()(c9)\n",
    "    c9 = Dropout(0.1) (c9)\n",
    "    c9 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c9)\n",
    "    c9 = BatchNormalization()(c9)\n",
    "\n",
    "    outputs = Conv2D(NUM_CLASSES, (1, 1), activation='softmax') (c9)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dice(im1, im2):\n",
    "    \"\"\"\n",
    "    Computes the Dice coefficient, a measure of set similarity.\n",
    "    Parameters\n",
    "    ----------\n",
    "    im1 : array-like, bool\n",
    "        Any array of arbitrary size. If not boolean, will be converted.\n",
    "    im2 : array-like, bool\n",
    "        Any other array of identical size. If not boolean, will be converted.\n",
    "    Returns\n",
    "    -------\n",
    "    dice : float\n",
    "        Dice coefficient as a float on range [0,1].\n",
    "        Maximum similarity = 1\n",
    "        No similarity = 0\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    The order of inputs for `dice` is irrelevant. The result will be\n",
    "    identical if `im1` and `im2` are switched.\n",
    "    \"\"\"\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "\n",
    "    if im1.shape != im2.shape:\n",
    "        raise ValueError(\"Shape mismatch: im1 and im2 must have the same shape.\")\n",
    "\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(target, prediction):\n",
    "    \n",
    "    target = np.asarray(target).astype(np.bool)\n",
    "    prediction = np.asarray(prediction).astype(np.bool)\n",
    "    \n",
    "    intersection = np.logical_and(target, prediction)\n",
    "    union = np.logical_or(target, prediction)\n",
    "    iou_score = np.sum(intersection) / np.sum(union)\n",
    "    return iou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_imgs(frame, prediction=None, groundtruth=None):\n",
    "    \n",
    "    if prediction is None:\n",
    "        prediction = groundtruth\n",
    "    \n",
    "    if groundtruth is None:\n",
    "        groundtruth = prediction\n",
    "    \n",
    "    overlayed = np.clip(np.dstack([0.75*frame + prediction,\n",
    "                                     0.75*frame,\n",
    "                                     0.75*frame + groundtruth]), a_min=0, a_max=1)\n",
    "    \n",
    "    return overlayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(IMG_HEIGHT, IMG_WIDTH, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_frames_indices = list(data[0]['original_frames'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame, label = augment_frame(data[0]['original_frames'][annotated_frames_indices[0]], \n",
    "                             data[0]['annotated_frames'][annotated_frames_indices[0]], is_train=False)\n",
    "plt.imshow(frame, cmap='gray')\n",
    "#plt.imshow(label, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame, label = augment_frame(data[0]['original_frames'][annotated_frames_indices[0]], \n",
    "                             data[0]['annotated_frames'][annotated_frames_indices[0]], is_train=True)\n",
    "\n",
    "plt.imshow(frame, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "random.shuffle(data)\n",
    "train_dataset = get_dataset(data[0:int(0.8*len(data))])\n",
    "val_dataset = get_dataset(data[int(0.8*len(data)):], is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model (visualize with weights and biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"seg_model_best.h5\", save_best_only=True),\n",
    "    tf.keras.callbacks.EarlyStopping(patience=15, mode=\"auto\",restore_best_weights=True, verbose=True)\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss = \"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "history = model.fit(train_dataset, \n",
    "                    epochs=epochs, \n",
    "                    callbacks=callbacks, \n",
    "                    validation_data=val_dataset,\n",
    "                    verbose=True,\n",
    "                    steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('seg_model_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = list(train_dataset.take(10).as_numpy_iterator())\n",
    "frames = np.vstack([f for f, l in inputs])\n",
    "labels = np.vstack([l for f, l in inputs])\n",
    "predictions = model.predict(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for f, l, p in zip(frames, labels, predictions):\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(16,6))\n",
    "    \n",
    "    axs[0].imshow(f[...], cmap='gray')\n",
    "    axs[0].imshow(l[..., 0], alpha=0.1)\n",
    "    axs[1].imshow(p[..., 0])\n",
    "    axs[2].imshow(np.abs(l[..., 0] - (p[..., 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make video (prediction and prediction overlayed on real) in real size of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = data[int(0.8*len(data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for p in validation_data:\n",
    "    \n",
    "    print(p['pat'])\n",
    "        \n",
    "    video = get_video_as_array(p['orig_video_path'], do_resize=False)\n",
    "    orig_height, orig_width = video.shape[1:3]\n",
    "    \n",
    "    predictions = []\n",
    "    frames = []\n",
    "    overlays = []\n",
    "    for f in video:\n",
    "        orig_frame = f[..., 0]\n",
    "        \n",
    "        frames.append(orig_frame)\n",
    "        \n",
    "        label = orig_frame\n",
    "        \n",
    "        frame = resize(orig_frame, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "        frame, label = augment_frame(frame, label, is_train=False)\n",
    "        frame = frame[np.newaxis, ..., np.newaxis]\n",
    "        \n",
    "        prediction = model.predict(frame, verbose=False)\n",
    "        \n",
    "        # thresholding prediction\n",
    "        prediction = prediction[0, ..., 1] > 0.5\n",
    "        \n",
    "        # resize to original\n",
    "        prediction = resize(prediction, (orig_height, orig_width), mode='constant', preserve_range=True)    \n",
    "        predictions.append(prediction)\n",
    "        \n",
    "        overlayed = overlay_imgs(orig_frame/255.0, prediction=prediction*1.0, groundtruth=None)\n",
    "        overlays.append(overlayed*255)\n",
    "        \n",
    "    \n",
    "    # save as gif\n",
    "    save_path = 'videos'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    file = os.path.join(save_path, p['pat'] + \"_original.avi\")\n",
    "    imageio.mimsave(file, [np.asarray(frame, dtype=np.uint8) for frame in frames],fps=25)\n",
    "    \n",
    "    file = os.path.join(save_path, p['pat'] + \"_prediction.avi\")\n",
    "    imageio.mimsave(file, [np.asarray(p*255, dtype=np.uint8) for p in predictions],fps=25)\n",
    "    \n",
    "        \n",
    "    file = os.path.join(save_path, p['pat'] + \"_overlayed.avi\")\n",
    "    imageio.mimsave(file, [np.asarray(o, dtype=np.uint8) for o in overlays],fps=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
